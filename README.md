# ğŸš€ Proyecto 3 â€” Arquitectura Batch para Big Data  
**Curso:** TÃ³picos Especiales en TelemÃ¡tica  
**Universidad EAFIT**  

**Equipo:**  
- JosÃ© Alejandro Duque  
- Pedro Saldarriaga  
- Juan Pablo Zuluaga  

---

## ğŸ“– DescripciÃ³n General

Este proyecto desarrolla un **pipeline batch completo** para el procesamiento de datos econÃ³micos, desplegado en servicios cloud de **AWS**.  

El pipeline sigue todas las etapas del **ciclo de vida del dato**:

1ï¸âƒ£ **Captura**  
2ï¸âƒ£ **Ingesta en S3**  
3ï¸âƒ£ **Procesamiento ETL (Spark sobre EMR)**  
4ï¸âƒ£ **GeneraciÃ³n de datasets refinados** (Parquet / CSV / JSON)  
5ï¸âƒ£ **ExposiciÃ³n de resultados (URL pre-firmada)**  

---

## ğŸ—ºï¸ Arquitectura del Sistema

### Diagrama General

![image](https://github.com/user-attachments/assets/adb646e1-5c5b-46a5-89a1-61fb9409acf4)  

![image](https://github.com/user-attachments/assets/e582f511-0c58-47c0-b389-7866fbc3b2d6)  

### TecnologÃ­as y Servicios

| TecnologÃ­a | Uso |
|------------|-----|
| **AWS S3** | Almacenamiento en zonas `raw/`, `trusted/` (Parquet), `refined/` (CSV + JSON) |
| **AWS EMR (Elastic MapReduce)** | Procesamiento distribuido con Spark |
| **Apache Spark** | ETL: limpieza, transformaciÃ³n, agregaciÃ³n (con formato Parquet columnar) |
| **Athena** | ValidaciÃ³n de resultados mediante consultas SQL |
| **Bash** | AutomatizaciÃ³n con `run_pipeline.sh` |

---

## âš™ï¸ EjecuciÃ³n del Pipeline

### ğŸš€ Prerrequisitos

- Bucket S3 creado (ejemplo: `juanzuluaga-proyecto3`)  
- ClÃºster EMR en estado `WAITING`  
- CSV de entrada ubicados en `~/`:

  - `inflacion_multi_country.csv`  
  - `inversion_extranjera_multi_country.csv`  
  - `indice_precios_consumidor.csv`  
  - `tasas_interes_politica.csv`  

---

### ğŸ“‹ Pasos de EjecuciÃ³n

1ï¸âƒ£ **Subir scripts Spark actualizados a S3**  
2ï¸âƒ£ **Ingestar datos a zona `raw/`**  
3ï¸âƒ£ **Lanzar Steps en EMR**:  

    - ETL â†’ genera zona `trusted/` (Parquet)  
    - AgregaciÃ³n â†’ genera zona `refined/` (CSV + JSON)  

4ï¸âƒ£ **Publicar URL pre-firmada con resultados**  

---

### ğŸ–¥ï¸ Comando Ãšnico

```bash
./run_pipeline.sh

```
Este script realiza automÃ¡ticamente:

âœ… Subida de scripts  
âœ… Ingesta a S3  
âœ… EjecuciÃ³n de Steps en EMR  
âœ… Monitoreo de ejecuciÃ³n  
âœ… PresentaciÃ³n de URL pre-firmada (vÃ¡lida por 7 dÃ­as)  

---

## ğŸ“ Uso de Formato Parquet

Utilizamos **Parquet** en `trusted/` por sus ventajas:

- Formato columnar  
- Alta compresiÃ³n  
- Optimizado para consultas analÃ­ticas  
- Compatible con Spark y Athena  

### Ejemplo de estructura en `trusted/`:

```
trusted/indicator=inflacion_anual/country=Colombia/part-.parquet
trusted/indicator=inversion_extranjera_directa/country=Brasil/part-.parquet
```


---

## ğŸš§ Retos Abordados

- Manejo de columnas con tildes y espacios en los CSV  
- CoordinaciÃ³n entre zonas `raw â†’ trusted â†’ refined`  
- DepuraciÃ³n de errores en Steps de EMR  
- AdaptaciÃ³n a restricciones de AWS Academy  
- ConstrucciÃ³n de un pipeline **100% automatizado**  

---

## âœ… Estado Final del Proyecto

- Pipeline probado y funcional en AWS  
- Resultados expuestos vÃ­a URL pre-firmada  
- EjecuciÃ³n automÃ¡tica con `run_pipeline.sh`  
- Listo para **escalado** o integraciÃ³n con nuevas fuentes de datos  

---

Â¡Gracias por revisar nuestro proyecto! ğŸš€âœ¨  
